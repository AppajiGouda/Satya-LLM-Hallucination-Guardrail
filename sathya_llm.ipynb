{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "gpuType": "T4",
      "provenance": []
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers"
      ],
      "metadata": {
        "id": "kMKPcBiXv6uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/google/gemma-2b\n",
        "\n",
        "‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/google/gemma-2b)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"
      ],
      "metadata": {
        "id": "PQW2ixwWv6uz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model you are trying to use is gated. Please make sure you have access to it by visiting the model page.To run inference, either set HF_TOKEN in your environment variables/ Secrets or run the following cell to login. ü§ó"
      ],
      "metadata": {
        "id": "8XTNHilzv6u1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "2JHsvp-2v6u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"google/gemma-2b\")"
      ],
      "metadata": {
        "id": "fM7mP-XBv6u2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")"
      ],
      "metadata": {
        "id": "HqQo7ubcv6u2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()\n",
        "\n"
      ],
      "metadata": {
        "id": "rlZ-C9y9xL-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q -U torch transformers accelerate bitsandbytes\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "model_id = \"google/gemma-2b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config={\"load_in_4bit\": True}\n",
        ")\n",
        "\n",
        "\n",
        "def interrogate_model(question, num_samples=5):\n",
        "    print(f\"üïµÔ∏è INTERROGATING: '{question}'\\n\")\n",
        "\n",
        "    prompt = f\"Question: {question}\\nAnswer in one short sentence:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    responses = []\n",
        "    for i in range(num_samples):\n",
        "\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            temperature=1.0,\n",
        "            do_sample=True,\n",
        "            top_k=50\n",
        "        )\n",
        "        ans = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
        "        responses.append(ans)\n",
        "        print(f\"  Sample {i+1}: {ans}\")\n",
        "\n",
        "    return responses\n",
        "\n",
        "\n",
        "trick_question = \"Who was the first person to land on Mars?\"\n",
        "samples = interrogate_model(trick_question)\n",
        "\n",
        "\n",
        "fact_question = \"What is the capital of France?\"\n",
        "samples_fact = interrogate_model(fact_question)\n"
      ],
      "metadata": {
        "id": "PxTQp8r6xfFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q sentence-transformers\n",
        "\n",
        "from sentence_transformers import CrossEncoder\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "print(\"Loading the NLI Judge (DeBERTa)...\")\n",
        "nli_model = CrossEncoder('cross-encoder/nli-deberta-v3-small')\n",
        "\n",
        "def calculate_consistency(samples):\n",
        "    print(\"\\n‚öñÔ∏è CALCULATING SEMANTIC CONSISTENCY (Contradiction-Based)...\")\n",
        "    reference = samples[0]\n",
        "    contradiction_scores = []\n",
        "\n",
        "    print(f\"Reference Story: '{reference}'\")\n",
        "\n",
        "    for i in range(1, len(samples)):\n",
        "        hypothesis = samples[i]\n",
        "\n",
        "\n",
        "        scores = nli_model.predict([reference, hypothesis])\n",
        "\n",
        "\n",
        "        probs = np.exp(scores) / np.sum(np.exp(scores))\n",
        "\n",
        "\n",
        "        contradiction_prob = probs[0]\n",
        "        contradiction_scores.append(contradiction_prob)\n",
        "\n",
        "\n",
        "        status = \"‚ùå CONFLICTS\" if contradiction_prob > 0.5 else \"‚úÖ AGREES\"\n",
        "        print(f\"  vs Sample {i+1}: {status} (Contradiction Score: {contradiction_prob:.2f})\")\n",
        "\n",
        "\n",
        "    mean_contradiction = np.mean(contradiction_scores)\n",
        "    final_truth_score = 1.0 - mean_contradiction\n",
        "\n",
        "    print(f\"\\n=> FINAL TRUTH SCORE: {final_truth_score:.2f} / 1.0\")\n",
        "    if final_truth_score > 0.60:\n",
        "        print(\"Verdict: üü¢ THE MODEL IS CONFIDENT (Consistent Story)\")\n",
        "    else:\n",
        "        print(\"Verdict: üî¥ HIGH UNCERTAINTY (Potential Hallucination/Lying!)\")\n",
        "\n",
        "    return final_truth_score\n",
        "\n",
        "\n",
        "calculate_consistency(france_answers)\n",
        "\n",
        "\n",
        "france_answers = [\n",
        "    \"Paris. The capital of France is Paris, which is a major city in the country.\",\n",
        "    \"Paris. Paris is the capital of France and is a popular tourist destination.\",\n",
        "    \"Paris is the capital of France. The capital of France is Paris, and it is in the north of Europe.\",\n",
        "    \"Paris. Paris is the capital of France, a country in Europe.\",\n",
        "    \"Paris is the capital of France. Please note that there are several other important cities in France, but Paris is the most important and well-known\"\n",
        "]\n",
        "\n",
        "calculate_consistency(france_answers)"
      ],
      "metadata": {
        "id": "XEAhPPh1xOf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "\n",
        "print(\"Loading REBEL (Triplet Extractor)...\")\n",
        "tokenizer_rebel = AutoTokenizer.from_pretrained('Babelscape/rebel-large')\n",
        "model_rebel = AutoModelForSeq2SeqLM.from_pretrained('Babelscape/rebel-large')\n",
        "\n",
        "\n",
        "triplet_extractor = pipeline(\n",
        "    'text-generation',\n",
        "    model=model_rebel,\n",
        "    tokenizer=tokenizer_rebel\n",
        ")\n",
        "\n",
        "\n",
        "def extract_triplets(text):\n",
        "    print(f\"\\nüî™ EXTRACTING TRIPLETS FROM: '{text}'\")\n",
        "\n",
        "\n",
        "    output = triplet_extractor(\n",
        "        text,\n",
        "        return_tensors=True\n",
        "    )\n",
        "\n",
        "\n",
        "    generated_token_ids = output[0][\"generated_token_ids\"]\n",
        "\n",
        "\n",
        "    extracted_text = triplet_extractor.tokenizer.batch_decode([generated_token_ids])\n",
        "\n",
        "\n",
        "    extracted_triplets = triplet_extractor.tokenizer.decode(\n",
        "        generated_token_ids,\n",
        "        skip_special_tokens=False\n",
        "    )\n",
        "\n",
        "    print(f\"  Raw Output: {extracted_triplets}\")\n",
        "    return extracted_triplets\n",
        "\n",
        "\n",
        "test_sentence = \"Paris is the capital of France and is a popular tourist destination.\"\n",
        "extract_triplets(test_sentence)"
      ],
      "metadata": {
        "id": "Bslk6RO7xRx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "\n",
        "def parse_rebel_output(raw_text):\n",
        "    triplets = []\n",
        "\n",
        "    clean_text = raw_text.replace('<s>', '').replace('</s>', '').strip()\n",
        "\n",
        "\n",
        "    chunks = clean_text.split('<triplet>')\n",
        "    for chunk in chunks:\n",
        "        if not chunk.strip(): continue\n",
        "        try:\n",
        "\n",
        "            head_part, rest = chunk.split('<subj>')\n",
        "            tail_part, rel_part = rest.split('<obj>')\n",
        "\n",
        "            triplets.append({\n",
        "                \"Subject\": head_part.strip(),\n",
        "                \"Object\": tail_part.strip(),\n",
        "                \"Relation\": rel_part.strip()\n",
        "            })\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "    return triplets\n",
        "\n",
        "\n",
        "def verify_fact_sparql(subject, object_):\n",
        "    print(f\"\\nüèõÔ∏è THE JUDGE: Verifying connection between '{subject}' and '{object_}'...\")\n",
        "\n",
        "    url = 'https://query.wikidata.org/sparql'\n",
        "\n",
        "\n",
        "    headers = {\n",
        "        'User-Agent': 'ProjectSatya/1.0 (Student_Researcher_Test)',\n",
        "        'Accept': 'application/sparql-results+json'\n",
        "    }\n",
        "\n",
        "    query = f\"\"\"\n",
        "    SELECT ?item ?itemLabel ?valLabel WHERE {{\n",
        "      ?item rdfs:label \"{subject}\"@en.\n",
        "      ?item ?prop ?val.\n",
        "      ?val rdfs:label \"{object_}\"@en.\n",
        "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "    }}\n",
        "    LIMIT 5\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "\n",
        "        r = requests.get(url, params={'query': query}, headers=headers)\n",
        "\n",
        "\n",
        "        if r.status_code != 200:\n",
        "            print(f\"  ‚ö†Ô∏è Blocked! HTTP Status: {r.status_code}\")\n",
        "            return 0.0\n",
        "\n",
        "        data = r.json()\n",
        "        results = data['results']['bindings']\n",
        "\n",
        "        if len(results) > 0:\n",
        "            print(\"  ‚úÖ VERIFIED: Found a direct relationship in the Knowledge Graph!\")\n",
        "            return 1.0\n",
        "        else:\n",
        "            print(\"  ‚ùå UNVERIFIED: No direct relationship found. Potential Hallucination.\")\n",
        "            return 0.0\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è API Error: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "raw_rebel_output = \"<s><triplet> Paris <subj> France <obj> country <triplet> France <subj> Paris <obj> capital</s>\"\n",
        "\n",
        "print(\"1. Parsing Triplets...\")\n",
        "parsed_facts = parse_rebel_output(raw_rebel_output)\n",
        "for fact in parsed_facts:\n",
        "    print(f\"  - {fact}\")\n",
        "\n",
        "print(\"\\n2. Querying Knowledge Graph...\")\n",
        "\n",
        "test_subject = parsed_facts[0]['Subject']\n",
        "test_object = parsed_facts[0]['Object']\n",
        "\n",
        "truth_score = verify_fact_sparql(test_subject, test_object)"
      ],
      "metadata": {
        "id": "qHDsC9k9xVGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def project_satya_pipeline(question):\n",
        "    print(f\"\\n========================================\")\n",
        "    print(f\"üöÄ PROJECT SATYA FACT-CHECKING PIPELINE\")\n",
        "    print(f\"Query: '{question}'\")\n",
        "    print(f\"========================================\")\n",
        "\n",
        "\n",
        "    samples = interrogate_model(question, num_samples=3)\n",
        "    main_answer = samples[0]\n",
        "\n",
        "\n",
        "    consistency_score = calculate_consistency(samples)\n",
        "\n",
        "    if consistency_score < 0.60:\n",
        "        print(\"\\n‚ùå PIPELINE HALTED: The LLM is hallucinating (Internal Contradiction).\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n‚úÖ Internal Consistency Passed. Proceeding to Knowledge Graph Verification...\")\n",
        "\n",
        "\n",
        "    raw_triplets_output = extract_triplets(main_answer)\n",
        "\n",
        "\n",
        "    if isinstance(raw_triplets_output, list) and 'generated_text' in raw_triplets_output[0]:\n",
        "        triplet_text = raw_triplets_output[0]['generated_text']\n",
        "    else:\n",
        "        triplet_text = str(raw_triplets_output)\n",
        "\n",
        "    parsed_facts = parse_rebel_output(triplet_text)\n",
        "\n",
        "    if not parsed_facts:\n",
        "        print(\"\\n‚ö†Ô∏è No extractable facts found in the answer. Cannot verify against graph.\")\n",
        "        return\n",
        "\n",
        "\n",
        "    verified_count = 0\n",
        "    for fact in parsed_facts:\n",
        "        subj, obj = fact['Subject'], fact['Object']\n",
        "        score = verify_fact_sparql(subj, obj)\n",
        "        verified_count += score\n",
        "\n",
        "    final_kg_score = verified_count / len(parsed_facts) if len(parsed_facts) > 0 else 0.0\n",
        "\n",
        "    print(f\"\\n========================================\")\n",
        "    print(f\"üìä FINAL REPORT\")\n",
        "    print(f\"Answer: {main_answer}\")\n",
        "    print(f\"Internal Consistency: {consistency_score*100:.1f}%\")\n",
        "    print(f\"Knowledge Graph Verification: {final_kg_score*100:.1f}%\")\n",
        "\n",
        "    if final_kg_score > 0.5:\n",
        "        print(f\"Verdict: üü¢ GROUNDED FACT (Safe to display to user)\")\n",
        "    else:\n",
        "        print(f\"Verdict: üî¥ HALLUCINATION (Failed External Verification)\")\n",
        "    print(f\"========================================\")\n",
        "\n",
        "\n",
        "print(\"\\n--- RUNNING PIPELINE ON A KNOWN FACT ---\")\n",
        "project_satya_pipeline(\"The capital city of Japan is\")\n",
        "\n",
        "print(\"\\n--- RUNNING PIPELINE ON A HALLUCINATION TRAP ---\")\n",
        "project_satya_pipeline(\"The person who invented the internet in 1842 was\")"
      ],
      "metadata": {
        "id": "dj-RSfC-0PdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "us_law_queries = [\n",
        "    \"Under the US Constitution, what does the Fifth Amendment protect a citizen from?\",\n",
        "    \"What is the maximum number of years a US President can serve?\",\n",
        "    \"Who has the explicit power to declare war under US Federal Law?\"\n",
        "]\n",
        "\n",
        "\n",
        "indian_law_queries = [\n",
        "    \"Under the Indian Constitution, what are the exact reasonable restrictions on Article 19?\",\n",
        "    \"What is the maximum duration of President's Rule in a state under Article 356?\",\n",
        "    \"Under the new Bharatiya Nyaya Sanhita (BNS) of 2024, what is the specific penalty for mob lynching?\"\n",
        "]\n",
        "\n",
        "def run_geographical_bias_audit(queries, domain_name):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"üåç INITIATING AUDIT: {domain_name.upper()}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    domain_scores = []\n",
        "\n",
        "    for query in queries:\n",
        "        # 1. Ask the model 5 times\n",
        "        samples = interrogate_model(query, num_samples=5)\n",
        "\n",
        "        # 2. Check how much it contradicts itself\n",
        "        score = calculate_consistency(samples)\n",
        "        domain_scores.append(score)\n",
        "\n",
        "    # Calculate the average confidence for this domain\n",
        "    avg_score = np.mean(domain_scores)\n",
        "    print(f\"\\nüìä {domain_name} AVERAGE TRUTH SCORE: {avg_score*100:.1f}%\")\n",
        "    return avg_score\n",
        "\n",
        "# 2. Execute the Experiment\n",
        "print(\"Starting the Comparative Experiment...\")\n",
        "us_score = run_geographical_bias_audit(us_law_queries, \"US Law\")\n",
        "indian_score = run_geographical_bias_audit(indian_law_queries, \"Indian Law\")\n",
        "\n",
        "# 3. The Final Research Conclusion\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"üèÜ EXPERIMENT RESULTS\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"US Law Average Consistency:     {us_score*100:.1f}%\")\n",
        "print(f\"Indian Law Average Consistency: {indian_score*100:.1f}%\")\n",
        "\n",
        "gap = (us_score - indian_score) * 100\n",
        "print(f\"\\nTHE KNOWLEDGE GAP: {gap:.1f}%\")\n",
        "\n",
        "if gap > 15.0:\n",
        "    print(\"CONCLUSION: Hypothesis PROVEN. The model exhibits significant geographical data bias, resulting in severe hallucinations on Indian Legal queries.\")\n",
        "else:\n",
        "    print(\"CONCLUSION: Hypothesis REJECTED. The model handles both domains with similar consistency.\")"
      ],
      "metadata": {
        "id": "WyRbg1drUvfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ViYuDQrKbvfD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}